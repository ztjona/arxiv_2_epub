@article{rajamanoharan2024improving,
  title={Improving Dictionary Learning with Gated Sparse Autoencoders},
  author={Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Shah, Rohin and Nanda, Neel},
  journal={arXiv preprint arXiv:2404.16014},
  year={2024},
  url={https://arxiv.org/abs/2404.16014}
}

@article{gao2024scaling,
  title={Scaling and evaluating sparse autoencoders},
  author={Gao, Leo and Dupr{\'e} la Tour, Tom and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  journal={arXiv preprint arXiv:2406.04093},
  year={2024},
  url={https://arxiv.org/abs/2406.04093}
}

@misc{bussmann2024batchtopk,
  title={BatchTopK: A Simple Improvement for TopK-SAEs},
  author={Bussmann, Bart and Leask, Patrick and Nanda, Neel},
  year={2024},
  url={https://www.alignmentforum.org/posts/Nkx6yWZNbAsfvic98/batchtopk-a-simple-improvement-for-topk-saes}
}

@misc{taggart2024prolu,
  title={ProLU: A Nonlinearity for Sparse Autoencoders},
  author={Taggart, Glen M.},
  year={2024},
  note={\url{https://www.alignmentforum.org/posts/HEpufTdakGTTKgoYF/prolu-a-nonlinearity-for-sparse-autoencoders}}
}

@misc{ayonrinde2024adaptivesparseallocationmutual,
      title={Adaptive Sparse Allocation with Mutual Choice \& Feature Choice Sparse Autoencoders}, 
      author={Kola Ayonrinde},
      year={2024},
      eprint={2411.02124},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.02124}, 
}

@misc{marks2024enhancingneuralnetworkinterpretability,
      title={Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders}, 
      author={Luke Marks and Alasdair Paren and David Krueger and Fazl Barez},
      year={2024},
      eprint={2411.01220},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.01220}, 
}

@misc{chanin2024absorptionstudyingfeaturesplitting,
      title={A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders}, 
      author={David Chanin and James Wilken-Smith and Tomáš Dulka and Hardik Bhatnagar and Joseph Bloom},
      year={2024},
      eprint={2409.14507},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.14507}, 
}

@misc{farrell2024applyingsparseautoencodersunlearn,
      title={Applying sparse autoencoders to unlearn knowledge in language models}, 
      author={Eoin Farrell and Yeu-Tong Lau and Arthur Conmy},
      year={2024},
      eprint={2410.19278},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.19278}, 
}

@misc{paulo2024automaticallyinterpretingmillionsfeatures,
      title={Automatically Interpreting Millions of Features in Large Language Models}, 
      author={Gonçalo Paulo and Alex Mallen and Caden Juang and Nora Belrose},
      year={2024},
      eprint={2410.13928},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.13928}, 
}

@misc{marks2024sparsefeaturecircuitsdiscovering,
      title={Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models}, 
      author={Samuel Marks and Can Rager and Eric J. Michaud and Yonatan Belinkov and David Bau and Aaron Mueller},
      year={2024},
      eprint={2403.19647},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.19647}, 
}



@misc{cunningham2023sparseautoencodershighlyinterpretable,
      title={Sparse Autoencoders Find Highly Interpretable Features in Language Models}, 
      author={Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey},
      year={2023},
      eprint={2309.08600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.08600}, 
}

@article{mudide2024efficient,
  title={Efficient Dictionary Learning with Switch Sparse Autoencoders},
  author={Mudide, Anish and Engels, Joshua and Michaud, Eric J and Tegmark, Max and Schroeder de Witt, Christian},
  journal={arXiv preprint arXiv:2410.08201},
  year={2024},
  url={https://arxiv.org/abs/2410.08201}
}

@article{rajamanoharan2024jumping,
  title={Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders},
  author={Rajamanoharan, Senthooran and Lieberum, Tom and Sonnerat, Nicolas and Conmy, Arthur and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Nanda, Neel},
  journal={arXiv preprint arXiv:2407.14435},
  year={2024},
  url={https://arxiv.org/abs/2407.14435}
}

@article{ghilardi2024efficient,
  title={Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups},
  author={Ghilardi, Davide and Belotti, Federico and Molinari, Marco},
  journal={arXiv preprint arXiv:2410.21508},
  year={2024},
  url={https://arxiv.org/abs/2410.21508}
}

@misc{makelov2024principledevaluationssparseautoencoders,
      title={Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control}, 
      author={Aleksandar Makelov and George Lange and Neel Nanda},
      year={2024},
      eprint={2405.08366},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.08366}, 
}

@misc{huang2024ravelevaluatinginterpretabilitymethods,
      title={RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations}, 
      author={Jing Huang and Zhengxuan Wu and Christopher Potts and Mor Geva and Atticus Geiger},
      year={2024},
      eprint={2402.17700},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17700}, 
}

@misc{heap2025sparseautoencodersinterpretrandomly,
      title={Sparse Autoencoders Can Interpret Randomly Initialized Transformers}, 
      author={Thomas Heap and Tim Lawson and Lucy Farnik and Laurence Aitchison},
      year={2025},
      eprint={2501.17727},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.17727}, 
}

@misc{chaudhary2024evaluatingopensourcesparseautoencoders,
      title={Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge in GPT-2 Small}, 
      author={Maheep Chaudhary and Atticus Geiger},
      year={2024},
      eprint={2409.04478},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.04478}, 
}

@misc{venhoff2024sagescalablegroundtruth,
      title={SAGE: Scalable Ground Truth Evaluations for Large Sparse Autoencoders}, 
      author={Constantin Venhoff and Anisoara Calinescu and Philip Torr and Christian Schroeder de Witt},
      year={2024},
      eprint={2410.07456},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.07456}, 
}

@misc{karvonen2024measuringprogressdictionarylearning,
      title={Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models}, 
      author={Adam Karvonen and Benjamin Wright and Can Rager and Rico Angell and Jannik Brinkmann and Logan Smith and Claudio Mayrink Verdun and David Bau and Samuel Marks},
      year={2024},
      eprint={2408.00113},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.00113}, 
}

@misc{marks2024dictionary_learning,
   title = {dictionary\_learning},
   author = {Samuel Marks and Adam Karvonen and Aaron Mueller},
   year = {2024},
   howpublished = {\url{https://github.com/saprmarks/dictionary_learning}},
}

@misc{anthropic_sae_2024,
  author       = "{Anthropic Interpretability Team}",
  title        = "Training Sparse Autoencoders",
  year         = "2024",
  howpublished = "\url{https://transformer-circuits.pub/2024/april-update/index.html#training-saes}",
  note         = "[Accessed January 20, 2025]"
}

@misc{bussmann2024matryoshka,
  author = {Bussmann, Bart and Leask, Patrick and Nanda, Neel},
  title = {Learning Multi-Level Features with Matryoshka SAEs},
  year = {2024},
  month = {December 19},
  note = {Alignment Forum},
  url = {https://www.alignmentforum.org/posts/rKM9b6B2LqwSB5ToN/learning-multi-level-features-with-matryoshka-saes}
}

@misc{neel_sae_replication,
    title={{Open Source Replication \& Commentary on Anthropic's Dictionary Learning Paper}},
    author={Nanda, Neel},
    year={2023},
    month={Oct},
    day={23},
    journal={AI Alignment Forum},
    url={https://www.alignmentforum.org/posts/aPTgTKC45dWvL9XBF/open-source-replication-and-commentary-on-anthropic-s},
}

@misc{lieberum2024gemmascopeopensparse,
      title={Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2}, 
      author={Tom Lieberum and Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Nicolas Sonnerat and Vikrant Varma and János Kramár and Anca Dragan and Rohin Shah and Neel Nanda},
      year={2024},
      eprint={2408.05147},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.05147}, 
}

@misc{biderman2023pythiasuiteanalyzinglarge,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.01373}, 
}

@misc{gemmateam2024gemma2improvingopen,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={{Gemma Team} and Morgane Riviere and Shreya Pathak and Pier Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and Léonard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ramé and Johan Ferret and Peter Liu and Pouya Tafti and Abe Friesen and Michelle Casbon and Sabela Ramos and Ravin Kumar and Charline Le Lan and Sammy Jerome and Anton Tsitsulin and Nino Vieillard and Piotr Stanczyk and Sertan Girgin and Nikola Momchev and Matt Hoffman and Shantanu Thakoor and Jean-Bastien Grill and Behnam Neyshabur and Olivier Bachem and Alanna Walton and Aliaksei Severyn and Alicia Parrish and Aliya Ahmad and Allen Hutchison and Alvin Abdagic and Amanda Carl and Amy Shen and Andy Brock and Andy Coenen and Anthony Laforge and Antonia Paterson and Ben Bastian and Bilal Piot and Bo Wu and Brandon Royal and Charlie Chen and Chintu Kumar and Chris Perry and Chris Welty and Christopher A. Choquette-Choo and Danila Sinopalnikov and David Weinberger and Dimple Vijaykumar and Dominika Rogozińska and Dustin Herbison and Elisa Bandy and Emma Wang and Eric Noland and Erica Moreira and Evan Senter and Evgenii Eltyshev and Francesco Visin and Gabriel Rasskin and Gary Wei and Glenn Cameron and Gus Martins and Hadi Hashemi and Hanna Klimczak-Plucińska and Harleen Batra and Harsh Dhand and Ivan Nardini and Jacinda Mein and Jack Zhou and James Svensson and Jeff Stanway and Jetha Chan and Jin Peng Zhou and Joana Carrasqueira and Joana Iljazi and Jocelyn Becker and Joe Fernandez and Joost van Amersfoort and Josh Gordon and Josh Lipschultz and Josh Newlan and Ju-yeong Ji and Kareem Mohamed and Kartikeya Badola and Kat Black and Katie Millican and Keelin McDonell and Kelvin Nguyen and Kiranbir Sodhia and Kish Greene and Lars Lowe Sjoesund and Lauren Usui and Laurent Sifre and Lena Heuermann and Leticia Lago and Lilly McNealus and Livio Baldini Soares and Logan Kilpatrick and Lucas Dixon and Luciano Martins and Machel Reid and Manvinder Singh and Mark Iverson and Martin Görner and Mat Velloso and Mateo Wirth and Matt Davidow and Matt Miller and Matthew Rahtz and Matthew Watson and Meg Risdal and Mehran Kazemi and Michael Moynihan and Ming Zhang and Minsuk Kahng and Minwoo Park and Mofi Rahman and Mohit Khatwani and Natalie Dao and Nenshad Bardoliwalla and Nesh Devanathan and Neta Dumai and Nilay Chauhan and Oscar Wahltinez and Pankil Botarda and Parker Barnes and Paul Barham and Paul Michel and Pengchong Jin and Petko Georgiev and Phil Culliton and Pradeep Kuppala and Ramona Comanescu and Ramona Merhej and Reena Jana and Reza Ardeshir Rokni and Rishabh Agarwal and Ryan Mullins and Samaneh Saadat and Sara Mc Carthy and Sarah Cogan and Sarah Perrin and Sébastien M. R. Arnold and Sebastian Krause and Shengyang Dai and Shruti Garg and Shruti Sheth and Sue Ronstrom and Susan Chan and Timothy Jordan and Ting Yu and Tom Eccles and Tom Hennigan and Tomas Kocisky and Tulsee Doshi and Vihan Jain and Vikas Yadav and Vilobh Meshram and Vishal Dharmadhikari and Warren Barkley and Wei Wei and Wenming Ye and Woohyun Han and Woosuk Kwon and Xiang Xu and Zhe Shen and Zhitao Gong and Zichuan Wei and Victor Cotruta and Phoebe Kirk and Anand Rao and Minh Giang and Ludovic Peran and Tris Warkentin and Eli Collins and Joelle Barral and Zoubin Ghahramani and Raia Hadsell and D. Sculley and Jeanine Banks and Anca Dragan and Slav Petrov and Oriol Vinyals and Jeff Dean and Demis Hassabis and Koray Kavukcuoglu and Clement Farabet and Elena Buchatskaya and Sebastian Borgeaud and Noah Fiedel and Armand Joulin and Kathleen Kenealy and Robert Dadashi and Alek Andreev},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}

@misc{chanin2024absorption,
      title={A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders}, 
      author={David Chanin and James Wilken-Smith and Tomáš Dulka and Hardik Bhatnagar and Joseph Bloom},
      year={2024},
      eprint={2409.14507},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.14507}
}

@misc{bussmann2024metasae,
      title={Showing SAE Latents Are Not Atomic Using Meta-SAEs}, 
      author={Bart Bussmann and Michael Pearce and Patrick Leask and Joseph Isaac Bloom and Lee Sharkey and Neel Nanda},
      year={2024},
      url={https://www.alignmentforum.org/posts/TMAmHh4DdMr4nCSr5/showing-sae-latents-are-not-atomic-using-meta-saes}
}

  @article{bricken2023monosemanticity,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
    }

@article{circuits2024august,
  title = {Circuits Updates — August 2024},
  author = {{Anthropic Interpretability Team}},
  journal = {Transformer Circuits Thread},
  year = {2024},
  url = {https://transformer-circuits.pub/2024/august-update/index.html}
}

@misc{wang2022interpretabilitywildcircuitindirect,
      title={Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small}, 
      author={Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
      year={2022},
      eprint={2211.00593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.00593}, 
}

@misc{gurnee2023findingneuronshaystackcase,
      title={Finding Neurons in a Haystack: Case Studies with Sparse Probing}, 
      author={Wes Gurnee and Neel Nanda and Matthew Pauly and Katherine Harvey and Dmitrii Troitskii and Dimitris Bertsimas},
      year={2023},
      eprint={2305.01610},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.01610}, 
}

@article{mueller2024quest,
  title={The quest for the right mediator: A history, survey, and theoretical grounding of causal interpretability},
  author={Mueller, Aaron and Brinkmann, Jannik and Li, Millicent and Marks, Samuel and Pal, Koyena and Prakash, Nikhil and Rager, Can and Sankaranarayanan, Aruna and Sharma, Arnab Sen and Sun, Jiuding and others},
  journal={arXiv preprint arXiv:2408.01416},
  year={2024}
}

@misc{engels2024languagemodelfeatureslinear,
      title={Not All Language Model Features Are Linear}, 
      author={Joshua Engels and Eric J. Michaud and Isaac Liao and Wes Gurnee and Max Tegmark},
      year={2024},
      eprint={2405.14860},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.14860}, 
}