\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{Anthropic Interpretability Team}(2024{\natexlab{a}})]{anthropic_sae_2024}
{Anthropic Interpretability Team}.
\newblock Training sparse autoencoders.
\newblock \url{https://transformer-circuits.pub/2024/april-update/index.html#training-saes}, 2024{\natexlab{a}}.
\newblock [Accessed January 20, 2025].

\bibitem[{Anthropic Interpretability Team}(2024{\natexlab{b}})]{circuits2024august}
{Anthropic Interpretability Team}.
\newblock Circuits updates — august 2024.
\newblock \emph{Transformer Circuits Thread}, 2024{\natexlab{b}}.
\newblock URL \url{https://transformer-circuits.pub/2024/august-update/index.html}.

\bibitem[Ayonrinde(2024)]{ayonrinde2024adaptivesparseallocationmutual}
Ayonrinde, K.
\newblock Adaptive sparse allocation with mutual choice \& feature choice sparse autoencoders, 2024.
\newblock URL \url{https://arxiv.org/abs/2411.02124}.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika, and van~der Wal]{biderman2023pythiasuiteanalyzinglarge}
Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O'Brien, K., Hallahan, E., Khan, M.~A., Purohit, S., Prashanth, U.~S., Raff, E., Skowron, A., Sutawika, L., and van~der Wal, O.
\newblock Pythia: A suite for analyzing large language models across training and scaling, 2023.
\newblock URL \url{https://arxiv.org/abs/2304.01373}.

\bibitem[Bricken et~al.(2023)Bricken, Templeton, Batson, Chen, Jermyn, Conerly, Turner, Anil, Denison, Askell, Lasenby, Wu, Kravec, Schiefer, Maxwell, Joseph, Hatfield-Dodds, Tamkin, Nguyen, McLean, Burke, Hume, Carter, Henighan, and Olah]{bricken2023monosemanticity}
Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer, N., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin, A., Nguyen, K., McLean, B., Burke, J.~E., Hume, T., Carter, S., Henighan, T., and Olah, C.
\newblock Towards monosemanticity: Decomposing language models with dictionary learning.
\newblock \emph{Transformer Circuits Thread}, 2023.
\newblock https://transformer-circuits.pub/2023/monosemantic-features/index.html.

\bibitem[Bussmann et~al.(2024{\natexlab{a}})Bussmann, Leask, and Nanda]{bussmann2024batchtopk}
Bussmann, B., Leask, P., and Nanda, N.
\newblock Batchtopk: A simple improvement for topk-saes, 2024{\natexlab{a}}.
\newblock URL \url{https://www.alignmentforum.org/posts/Nkx6yWZNbAsfvic98/batchtopk-a-simple-improvement-for-topk-saes}.

\bibitem[Bussmann et~al.(2024{\natexlab{b}})Bussmann, Leask, and Nanda]{bussmann2024matryoshka}
Bussmann, B., Leask, P., and Nanda, N.
\newblock Learning multi-level features with matryoshka saes, December 19 2024{\natexlab{b}}.
\newblock URL \url{https://www.alignmentforum.org/posts/rKM9b6B2LqwSB5ToN/learning-multi-level-features-with-matryoshka-saes}.
\newblock Alignment Forum.

\bibitem[Bussmann et~al.(2024{\natexlab{c}})Bussmann, Pearce, Leask, Bloom, Sharkey, and Nanda]{bussmann2024metasae}
Bussmann, B., Pearce, M., Leask, P., Bloom, J.~I., Sharkey, L., and Nanda, N.
\newblock Showing sae latents are not atomic using meta-saes, 2024{\natexlab{c}}.
\newblock URL \url{https://www.alignmentforum.org/posts/TMAmHh4DdMr4nCSr5/showing-sae-latents-are-not-atomic-using-meta-saes}.

\bibitem[Chanin et~al.(2024{\natexlab{a}})Chanin, Wilken-Smith, Dulka, Bhatnagar, and Bloom]{chanin2024absorption}
Chanin, D., Wilken-Smith, J., Dulka, T., Bhatnagar, H., and Bloom, J.
\newblock A is for absorption: Studying feature splitting and absorption in sparse autoencoders, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2409.14507}.

\bibitem[Chanin et~al.(2024{\natexlab{b}})Chanin, Wilken-Smith, Dulka, Bhatnagar, and Bloom]{chanin2024absorptionstudyingfeaturesplitting}
Chanin, D., Wilken-Smith, J., Dulka, T., Bhatnagar, H., and Bloom, J.
\newblock A is for absorption: Studying feature splitting and absorption in sparse autoencoders, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2409.14507}.

\bibitem[Chaudhary \& Geiger(2024)Chaudhary and Geiger]{chaudhary2024evaluatingopensourcesparseautoencoders}
Chaudhary, M. and Geiger, A.
\newblock Evaluating open-source sparse autoencoders on disentangling factual knowledge in gpt-2 small, 2024.
\newblock URL \url{https://arxiv.org/abs/2409.04478}.

\bibitem[Cunningham et~al.(2023)Cunningham, Ewart, Riggs, Huben, and Sharkey]{cunningham2023sparseautoencodershighlyinterpretable}
Cunningham, H., Ewart, A., Riggs, L., Huben, R., and Sharkey, L.
\newblock Sparse autoencoders find highly interpretable features in language models, 2023.
\newblock URL \url{https://arxiv.org/abs/2309.08600}.

\bibitem[Engels et~al.(2024)Engels, Michaud, Liao, Gurnee, and Tegmark]{engels2024languagemodelfeatureslinear}
Engels, J., Michaud, E.~J., Liao, I., Gurnee, W., and Tegmark, M.
\newblock Not all language model features are linear, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.14860}.

\bibitem[Farrell et~al.(2024)Farrell, Lau, and Conmy]{farrell2024applyingsparseautoencodersunlearn}
Farrell, E., Lau, Y.-T., and Conmy, A.
\newblock Applying sparse autoencoders to unlearn knowledge in language models, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.19278}.

\bibitem[Gao et~al.(2024)Gao, Dupr{\'e}~la Tour, Tillman, Goh, Troll, Radford, Sutskever, Leike, and Wu]{gao2024scaling}
Gao, L., Dupr{\'e}~la Tour, T., Tillman, H., Goh, G., Troll, R., Radford, A., Sutskever, I., Leike, J., and Wu, J.
\newblock Scaling and evaluating sparse autoencoders.
\newblock \emph{arXiv preprint arXiv:2406.04093}, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.04093}.

\bibitem[{Gemma Team} et~al.(2024){Gemma Team}, Riviere, Pathak, Sessa, Hardin, Bhupatiraju, Hussenot, Mesnard, Shahriari, Ramé, Ferret, Liu, Tafti, Friesen, Casbon, Ramos, Kumar, Lan, Jerome, Tsitsulin, Vieillard, Stanczyk, Girgin, Momchev, Hoffman, Thakoor, Grill, Neyshabur, Bachem, Walton, Severyn, Parrish, Ahmad, Hutchison, Abdagic, Carl, Shen, Brock, Coenen, Laforge, Paterson, Bastian, Piot, Wu, Royal, Chen, Kumar, Perry, Welty, Choquette-Choo, Sinopalnikov, Weinberger, Vijaykumar, Rogozińska, Herbison, Bandy, Wang, Noland, Moreira, Senter, Eltyshev, Visin, Rasskin, Wei, Cameron, Martins, Hashemi, Klimczak-Plucińska, Batra, Dhand, Nardini, Mein, Zhou, Svensson, Stanway, Chan, Zhou, Carrasqueira, Iljazi, Becker, Fernandez, van Amersfoort, Gordon, Lipschultz, Newlan, yeong Ji, Mohamed, Badola, Black, Millican, McDonell, Nguyen, Sodhia, Greene, Sjoesund, Usui, Sifre, Heuermann, Lago, McNealus, Soares, Kilpatrick, Dixon, Martins, Reid, Singh, Iverson, Görner, Velloso, Wirth, Davidow, Miller, Rahtz,
  Watson, Risdal, Kazemi, Moynihan, Zhang, Kahng, Park, Rahman, Khatwani, Dao, Bardoliwalla, Devanathan, Dumai, Chauhan, Wahltinez, Botarda, Barnes, Barham, Michel, Jin, Georgiev, Culliton, Kuppala, Comanescu, Merhej, Jana, Rokni, Agarwal, Mullins, Saadat, Carthy, Cogan, Perrin, Arnold, Krause, Dai, Garg, Sheth, Ronstrom, Chan, Jordan, Yu, Eccles, Hennigan, Kocisky, Doshi, Jain, Yadav, Meshram, Dharmadhikari, Barkley, Wei, Ye, Han, Kwon, Xu, Shen, Gong, Wei, Cotruta, Kirk, Rao, Giang, Peran, Warkentin, Collins, Barral, Ghahramani, Hadsell, Sculley, Banks, Dragan, Petrov, Vinyals, Dean, Hassabis, Kavukcuoglu, Farabet, Buchatskaya, Borgeaud, Fiedel, Joulin, Kenealy, Dadashi, and Andreev]{gemmateam2024gemma2improvingopen}
{Gemma Team}, Riviere, M., Pathak, S., Sessa, P.~G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Ramé, A., Ferret, J., Liu, P., Tafti, P., Friesen, A., Casbon, M., Ramos, S., Kumar, R., Lan, C.~L., Jerome, S., Tsitsulin, A., Vieillard, N., Stanczyk, P., Girgin, S., Momchev, N., Hoffman, M., Thakoor, S., Grill, J.-B., Neyshabur, B., Bachem, O., Walton, A., Severyn, A., Parrish, A., Ahmad, A., Hutchison, A., Abdagic, A., Carl, A., Shen, A., Brock, A., Coenen, A., Laforge, A., Paterson, A., Bastian, B., Piot, B., Wu, B., Royal, B., Chen, C., Kumar, C., Perry, C., Welty, C., Choquette-Choo, C.~A., Sinopalnikov, D., Weinberger, D., Vijaykumar, D., Rogozińska, D., Herbison, D., Bandy, E., Wang, E., Noland, E., Moreira, E., Senter, E., Eltyshev, E., Visin, F., Rasskin, G., Wei, G., Cameron, G., Martins, G., Hashemi, H., Klimczak-Plucińska, H., Batra, H., Dhand, H., Nardini, I., Mein, J., Zhou, J., Svensson, J., Stanway, J., Chan, J., Zhou, J.~P., Carrasqueira, J., Iljazi, J., Becker,
  J., Fernandez, J., van Amersfoort, J., Gordon, J., Lipschultz, J., Newlan, J., yeong Ji, J., Mohamed, K., Badola, K., Black, K., Millican, K., McDonell, K., Nguyen, K., Sodhia, K., Greene, K., Sjoesund, L.~L., Usui, L., Sifre, L., Heuermann, L., Lago, L., McNealus, L., Soares, L.~B., Kilpatrick, L., Dixon, L., Martins, L., Reid, M., Singh, M., Iverson, M., Görner, M., Velloso, M., Wirth, M., Davidow, M., Miller, M., Rahtz, M., Watson, M., Risdal, M., Kazemi, M., Moynihan, M., Zhang, M., Kahng, M., Park, M., Rahman, M., Khatwani, M., Dao, N., Bardoliwalla, N., Devanathan, N., Dumai, N., Chauhan, N., Wahltinez, O., Botarda, P., Barnes, P., Barham, P., Michel, P., Jin, P., Georgiev, P., Culliton, P., Kuppala, P., Comanescu, R., Merhej, R., Jana, R., Rokni, R.~A., Agarwal, R., Mullins, R., Saadat, S., Carthy, S.~M., Cogan, S., Perrin, S., Arnold, S. M.~R., Krause, S., Dai, S., Garg, S., Sheth, S., Ronstrom, S., Chan, S., Jordan, T., Yu, T., Eccles, T., Hennigan, T., Kocisky, T., Doshi, T., Jain, V., Yadav, V.,
  Meshram, V., Dharmadhikari, V., Barkley, W., Wei, W., Ye, W., Han, W., Kwon, W., Xu, X., Shen, Z., Gong, Z., Wei, Z., Cotruta, V., Kirk, P., Rao, A., Giang, M., Peran, L., Warkentin, T., Collins, E., Barral, J., Ghahramani, Z., Hadsell, R., Sculley, D., Banks, J., Dragan, A., Petrov, S., Vinyals, O., Dean, J., Hassabis, D., Kavukcuoglu, K., Farabet, C., Buchatskaya, E., Borgeaud, S., Fiedel, N., Joulin, A., Kenealy, K., Dadashi, R., and Andreev, A.
\newblock Gemma 2: Improving open language models at a practical size, 2024.
\newblock URL \url{https://arxiv.org/abs/2408.00118}.

\bibitem[Gurnee et~al.(2023)Gurnee, Nanda, Pauly, Harvey, Troitskii, and Bertsimas]{gurnee2023findingneuronshaystackcase}
Gurnee, W., Nanda, N., Pauly, M., Harvey, K., Troitskii, D., and Bertsimas, D.
\newblock Finding neurons in a haystack: Case studies with sparse probing, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.01610}.

\bibitem[Heap et~al.(2025)Heap, Lawson, Farnik, and Aitchison]{heap2025sparseautoencodersinterpretrandomly}
Heap, T., Lawson, T., Farnik, L., and Aitchison, L.
\newblock Sparse autoencoders can interpret randomly initialized transformers, 2025.
\newblock URL \url{https://arxiv.org/abs/2501.17727}.

\bibitem[Huang et~al.(2024)Huang, Wu, Potts, Geva, and Geiger]{huang2024ravelevaluatinginterpretabilitymethods}
Huang, J., Wu, Z., Potts, C., Geva, M., and Geiger, A.
\newblock Ravel: Evaluating interpretability methods on disentangling language model representations, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.17700}.

\bibitem[Karvonen et~al.(2024)Karvonen, Wright, Rager, Angell, Brinkmann, Smith, Verdun, Bau, and Marks]{karvonen2024measuringprogressdictionarylearning}
Karvonen, A., Wright, B., Rager, C., Angell, R., Brinkmann, J., Smith, L., Verdun, C.~M., Bau, D., and Marks, S.
\newblock Measuring progress in dictionary learning for language model interpretability with board game models, 2024.
\newblock URL \url{https://arxiv.org/abs/2408.00113}.

\bibitem[Lieberum et~al.(2024)Lieberum, Rajamanoharan, Conmy, Smith, Sonnerat, Varma, Kramár, Dragan, Shah, and Nanda]{lieberum2024gemmascopeopensparse}
Lieberum, T., Rajamanoharan, S., Conmy, A., Smith, L., Sonnerat, N., Varma, V., Kramár, J., Dragan, A., Shah, R., and Nanda, N.
\newblock Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2, 2024.
\newblock URL \url{https://arxiv.org/abs/2408.05147}.

\bibitem[Makelov et~al.(2024)Makelov, Lange, and Nanda]{makelov2024principledevaluationssparseautoencoders}
Makelov, A., Lange, G., and Nanda, N.
\newblock Towards principled evaluations of sparse autoencoders for interpretability and control, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.08366}.

\bibitem[Marks et~al.(2024{\natexlab{a}})Marks, Paren, Krueger, and Barez]{marks2024enhancingneuralnetworkinterpretability}
Marks, L., Paren, A., Krueger, D., and Barez, F.
\newblock Enhancing neural network interpretability with feature-aligned sparse autoencoders, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2411.01220}.

\bibitem[Marks et~al.(2024{\natexlab{b}})Marks, Karvonen, and Mueller]{marks2024dictionary_learning}
Marks, S., Karvonen, A., and Mueller, A.
\newblock dictionary\_learning.
\newblock \url{https://github.com/saprmarks/dictionary_learning}, 2024{\natexlab{b}}.

\bibitem[Marks et~al.(2024{\natexlab{c}})Marks, Rager, Michaud, Belinkov, Bau, and Mueller]{marks2024sparsefeaturecircuitsdiscovering}
Marks, S., Rager, C., Michaud, E.~J., Belinkov, Y., Bau, D., and Mueller, A.
\newblock Sparse feature circuits: Discovering and editing interpretable causal graphs in language models, 2024{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2403.19647}.

\bibitem[Mudide et~al.(2024)Mudide, Engels, Michaud, Tegmark, and Schroeder~de Witt]{mudide2024efficient}
Mudide, A., Engels, J., Michaud, E.~J., Tegmark, M., and Schroeder~de Witt, C.
\newblock Efficient dictionary learning with switch sparse autoencoders.
\newblock \emph{arXiv preprint arXiv:2410.08201}, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.08201}.

\bibitem[Nanda(2023)]{neel_sae_replication}
Nanda, N.
\newblock {Open Source Replication \& Commentary on Anthropic's Dictionary Learning Paper}, Oct 2023.
\newblock URL \url{https://www.alignmentforum.org/posts/aPTgTKC45dWvL9XBF/open-source-replication-and-commentary-on-anthropic-s}.

\bibitem[Paulo et~al.(2024)Paulo, Mallen, Juang, and Belrose]{paulo2024automaticallyinterpretingmillionsfeatures}
Paulo, G., Mallen, A., Juang, C., and Belrose, N.
\newblock Automatically interpreting millions of features in large language models, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.13928}.

\bibitem[Rajamanoharan et~al.(2024{\natexlab{a}})Rajamanoharan, Conmy, Smith, Lieberum, Varma, Kram{\'a}r, Shah, and Nanda]{rajamanoharan2024improving}
Rajamanoharan, S., Conmy, A., Smith, L., Lieberum, T., Varma, V., Kram{\'a}r, J., Shah, R., and Nanda, N.
\newblock Improving dictionary learning with gated sparse autoencoders.
\newblock \emph{arXiv preprint arXiv:2404.16014}, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2404.16014}.

\bibitem[Rajamanoharan et~al.(2024{\natexlab{b}})Rajamanoharan, Lieberum, Sonnerat, Conmy, Varma, Kram{\'a}r, and Nanda]{rajamanoharan2024jumping}
Rajamanoharan, S., Lieberum, T., Sonnerat, N., Conmy, A., Varma, V., Kram{\'a}r, J., and Nanda, N.
\newblock Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders.
\newblock \emph{arXiv preprint arXiv:2407.14435}, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2407.14435}.

\bibitem[Taggart(2024)]{taggart2024prolu}
Taggart, G.~M.
\newblock Prolu: A nonlinearity for sparse autoencoders, 2024.
\newblock \url{https://www.alignmentforum.org/posts/HEpufTdakGTTKgoYF/prolu-a-nonlinearity-for-sparse-autoencoders}.

\bibitem[Venhoff et~al.(2024)Venhoff, Calinescu, Torr, and de~Witt]{venhoff2024sagescalablegroundtruth}
Venhoff, C., Calinescu, A., Torr, P., and de~Witt, C.~S.
\newblock Sage: Scalable ground truth evaluations for large sparse autoencoders, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.07456}.

\bibitem[Wang et~al.(2022)Wang, Variengien, Conmy, Shlegeris, and Steinhardt]{wang2022interpretabilitywildcircuitindirect}
Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J.
\newblock Interpretability in the wild: a circuit for indirect object identification in gpt-2 small, 2022.
\newblock URL \url{https://arxiv.org/abs/2211.00593}.

\end{thebibliography}
